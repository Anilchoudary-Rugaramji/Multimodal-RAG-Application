{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a24002d8",
   "metadata": {},
   "source": [
    "### 1. Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7781d0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80537a67",
   "metadata": {},
   "source": [
    "### 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1f89333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF partitioning\n",
    "from unstructured.partition.pdf import partition_pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5226fc1a",
   "metadata": {},
   "source": [
    "### 3. Partition PDF into Structured Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33524c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.Text at 0x27a34edbe70>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edbee0>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edb930>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edb9a0>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edba10>,\n",
       " <unstructured.documents.elements.Header at 0x27a2507c050>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edba80>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edbaf0>,\n",
       " <unstructured.documents.elements.Text at 0x27a3432d470>,\n",
       " <unstructured.documents.elements.Title at 0x27a34edbb60>,\n",
       " <unstructured.documents.elements.Title at 0x27a34edbbd0>,\n",
       " <unstructured.documents.elements.Title at 0x27a34edbc40>,\n",
       " <unstructured.documents.elements.Title at 0x27a345ca190>,\n",
       " <unstructured.documents.elements.Title at 0x27a3432d710>,\n",
       " <unstructured.documents.elements.Title at 0x27a34edbcb0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5b540>,\n",
       " <unstructured.documents.elements.Title at 0x27a2de5b5b0>,\n",
       " <unstructured.documents.elements.Title at 0x27a3432d630>,\n",
       " <unstructured.documents.elements.Title at 0x27a34edbd90>,\n",
       " <unstructured.documents.elements.EmailAddress at 0x27a33fc74d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5b9a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5ba80>,\n",
       " <unstructured.documents.elements.Title at 0x27a34edbd20>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5bd20>,\n",
       " <unstructured.documents.elements.Title at 0x27a34edb620>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4903d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490210>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490440>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b4904b0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490590>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b490670>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490750>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490f30>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a345ca970>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a34edb690>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4911d0>,\n",
       " <unstructured.documents.elements.Title at 0x27a2de5b3f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5b690>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5b770>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5b930>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5baf0>,\n",
       " <unstructured.documents.elements.Title at 0x27a2de5bcb0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4914e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490050>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490360>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490520>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b4906e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4908a0>,\n",
       " <unstructured.documents.elements.Footer at 0x27a2b490a60>,\n",
       " <unstructured.documents.elements.Image at 0x27a24ea0d10>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x27a25066f90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5b850>,\n",
       " <unstructured.documents.elements.Title at 0x27a2de5bbd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490980>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490280>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b490600>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490b40>,\n",
       " <unstructured.documents.elements.Footer at 0x27a2b490d00>,\n",
       " <unstructured.documents.elements.Title at 0x27a34edb700>,\n",
       " <unstructured.documents.elements.Title at 0x27a345e6040>,\n",
       " <unstructured.documents.elements.Image at 0x27a287f7a50>,\n",
       " <unstructured.documents.elements.Image at 0x27a2f310550>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5ba10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490c20>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b4902f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490de0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490fa0>,\n",
       " <unstructured.documents.elements.Formula at 0x27a24e78ef0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491240>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491400>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b491470>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491550>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4918d0>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edb770>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5b620>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5bd90>,\n",
       " <unstructured.documents.elements.Formula at 0x27a24f85040>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4907c0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491080>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b491320>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4916a0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b491630>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b491b70>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b491c50>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b491d30>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491e10>,\n",
       " <unstructured.documents.elements.Formula at 0x27a24cc2cf0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491da0>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b491e80>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491f60>,\n",
       " <unstructured.documents.elements.Footer at 0x27a2b4923c0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5b4d0>,\n",
       " <unstructured.documents.elements.Table at 0x27a24dcbce0>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b4900c0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491160>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4917f0>,\n",
       " <unstructured.documents.elements.Formula at 0x27a2b4fce50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491b00>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491cc0>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b491fd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492430>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492510>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4925f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4926d0>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edb460>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492740>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491a90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4915c0>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b491a20>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491ef0>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b4924a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492660>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b492890>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492970>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b492a50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492b30>,\n",
       " <unstructured.documents.elements.Formula at 0x27a2b4ffb50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492c80>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b492d60>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492e40>,\n",
       " <unstructured.documents.elements.Footer at 0x27a2b492f20>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x27a65b2bf00>,\n",
       " <unstructured.documents.elements.Table at 0x27a292839b0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490ec0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491be0>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b492580>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b492900>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492ac0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492c10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492dd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493000>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b4930e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4931c0>,\n",
       " <unstructured.documents.elements.Text at 0x27a345cadd0>,\n",
       " <unstructured.documents.elements.Footer at 0x27a2b493380>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493310>,\n",
       " <unstructured.documents.elements.Table at 0x27a29193bd0>,\n",
       " <unstructured.documents.elements.Title at 0x27a34edb4d0>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edb540>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edb5b0>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edb380>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edb3f0>,\n",
       " <unstructured.documents.elements.Text at 0x27a345e63c0>,\n",
       " <unstructured.documents.elements.Text at 0x27a345e6430>,\n",
       " <unstructured.documents.elements.Text at 0x27a345cb150>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493770>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493850>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493930>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b493a10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493af0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493bd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493cb0>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edb7e0>,\n",
       " <unstructured.documents.elements.Text at 0x27a33fcf690>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x27a2b38b9b0>,\n",
       " <unstructured.documents.elements.Table at 0x27a2b38a030>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492190>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4929e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493070>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b493460>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493620>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4937e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4939a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493b60>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493e00>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b493ee0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc050>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc130>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc210>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc2f0>,\n",
       " <unstructured.documents.elements.Footer at 0x27a2eebc3d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b493d20>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b4920b0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b493230>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b493700>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b493a80>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b493e70>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc360>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc1a0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc440>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc520>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc600>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc750>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc7c0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc8a0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc980>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebca60>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebcb40>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebcc20>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebcd00>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebcde0>,\n",
       " <unstructured.documents.elements.Footer at 0x27a2eebcec0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b492eb0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b493540>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b493c40>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebce50>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc280>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc590>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc6e0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc910>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebcad0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebcc90>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebcf30>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebd010>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebd0f0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebd1d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebd2b0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebd390>,\n",
       " <unstructured.documents.elements.Footer at 0x27a2eebd470>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b492cf0>,\n",
       " <unstructured.documents.elements.Image at 0x27a2be4c5f0>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x27a2b4ffe50>,\n",
       " <unstructured.documents.elements.Header at 0x27a24d0cfc0>,\n",
       " <unstructured.documents.elements.Image at 0x27a34008050>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493f50>,\n",
       " <unstructured.documents.elements.Text at 0x27a2de5be70>,\n",
       " <unstructured.documents.elements.Image at 0x27a2dea9a90>,\n",
       " <unstructured.documents.elements.Image at 0x27a2dea9630>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x27a2b4fef50>,\n",
       " <unstructured.documents.elements.Header at 0x27a292803b0>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Partition a PDF into elements: headers, paragraphs, images, tables, etc.\n",
    "#  Path to your PDF\n",
    "pdf_path = r\"C:\\Users\\Anilchoudary R\\Gitrepos\\Multimodal-RAG-Application\\data\\attention_paper.pdf\"\n",
    "\n",
    "#  Partition the PDF\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=pdf_path,\n",
    "    strategy=\"hi_res\",\n",
    "    extract_images_in_pdf=True,\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],\n",
    "    extract_image_block_to_payload=False,\n",
    "    extract_image_block_output_dir=\"extracted_docs\"\n",
    ")\n",
    "\n",
    "raw_pdf_elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797b20d",
   "metadata": {},
   "source": [
    "### 4. Categorize PDF Elements by Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7860106a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers: 3, Footers: 8, Titles: 40\n",
      "NarrativeTexts: 83, Text: 21, ListItems: 43\n",
      "Tables: 4, Images: 7\n"
     ]
    }
   ],
   "source": [
    "#  Organize extracted PDF elements by type\n",
    "\n",
    "Header, Footer, Title = [], [], []\n",
    "NarrativeTexts, Text, ListItem = [], [], []\n",
    "Images, Tables = [], []\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    t = str(type(element))\n",
    "    if \"Header\" in t:\n",
    "        Header.append(str(element))\n",
    "    elif \"Footer\" in t:\n",
    "        Footer.append(str(element))\n",
    "    elif \"Title\" in t:\n",
    "        Title.append(str(element))\n",
    "    elif \"NarrativeText\" in t:\n",
    "        NarrativeTexts.append(str(element))\n",
    "    elif \"Text\" in t:\n",
    "        Text.append(str(element))\n",
    "    elif \"ListItem\" in t:\n",
    "        ListItem.append(str(element))\n",
    "    elif \"Image\" in t:\n",
    "        Images.append(str(element))\n",
    "    elif \"Table\" in t:\n",
    "        Tables.append(str(element))\n",
    "\n",
    "#  Summary of extracted types\n",
    "print(f\"Headers: {len(Header)}, Footers: {len(Footer)}, Titles: {len(Title)}\")\n",
    "print(f\"NarrativeTexts: {len(NarrativeTexts)}, Text: {len(Text)}, ListItems: {len(ListItem)}\")\n",
    "print(f\"Tables: {len(Tables)}, Images: {len(Images)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f08c0",
   "metadata": {},
   "source": [
    "This block loops over all elements and sorts them into lists so you can handle text, tables, and images separately later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d981a6",
   "metadata": {},
   "source": [
    "### 5.  Helper Functions for encoding image binary first into base64 and then into python string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a8d7c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Helper to encode an image to base64\n",
    "import base64\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"\n",
    "    Encode an image as a base64 string.\n",
    "    \"\"\"\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca762d73",
   "metadata": {},
   "source": [
    "### 6. Helper Function for Image Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29acf6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Summarize an image using GPT-4 with vision\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def image_summarize(img_b64, prompt):\n",
    "    \"\"\"\n",
    "    Summarize a base64-encoded image with a text prompt using GPT-4 Vision.\n",
    "    \"\"\"\n",
    "    model = ChatOpenAI(model=\"gpt-4o\", max_tokens=1024)\n",
    "    response = model.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{img_b64}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc359e0",
   "metadata": {},
   "source": [
    "These helper functions encode an image to base64 and then use GPT-4 Vision to summarize it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bed601",
   "metadata": {},
   "source": [
    "### 7. Helper Function for generating Image Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0d0a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_based_summaries(image_paths, prompt):\n",
    "    \"\"\"\n",
    "    Generic helper: encode and summarize a list of image file paths using a given prompt.\n",
    "    Returns: (list of base64 strings, list of summaries)\n",
    "    \"\"\"\n",
    "    b64_list = []\n",
    "    summaries = []\n",
    "\n",
    "    for path in image_paths:\n",
    "        b64 = encode_image(path)\n",
    "        b64_list.append(b64)\n",
    "        summary = image_summarize(b64, prompt)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return b64_list, summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a892b3",
   "metadata": {},
   "source": [
    "### 8. Prompts for Tables and Images separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e006e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PROMPT = (\n",
    "    \"You are an assistant tasked with summarizing images for retrieval. \"\n",
    "    \"Describe the content of this image in detail.\"\n",
    ")\n",
    "\n",
    "TABLE_PROMPT = (\n",
    "    \"You are an assistant tasked with summarizing tables from images for retrieval. \"\n",
    "    \"Describe the data, key insights, and structure in detail.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb36fb",
   "metadata": {},
   "source": [
    "### 9. Generate Base64 and Summaries for Normal Images and Table Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1785c542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image summary: The image is a diagram representing the architecture of a Transformer model, commonly used in natural language processing. \n",
      "\n",
      "- **Left Section (Encoder):**\n",
      "  - At the bottom, it starts with \"Input Embedding\" where inputs are embedded.\n",
      "  - Positional encoding is applied before they enter the layers.\n",
      "  - The encoder consists of multiple identical layers (Nx) wrapping two main components: \"Multi-Head Attention\" and \"Feed Forward.\"\n",
      "  - Each component is followed by \"Add & Norm,\" indicating a residual connection and layer normalization.\n",
      "\n",
      "- **Right Section (Decoder):**\n",
      "  - Begins with \"Output Embedding\" and includes shifted outputs. \n",
      "  - Similar to the encoder, positional encoding is added.\n",
      "  - The decoder has more components per layer: \"Masked Multi-Head Attention\", \"Multi-Head Attention\" (taking input from the encoder), and \"Feed Forward,\" each followed by \"Add & Norm.\"\n",
      "  \n",
      "- **Top Section:**\n",
      "  - The final layers include a \"Linear\" transformation followed by a \"Softmax\" to convert model outputs into probabilities.\n",
      "\n",
      "Arrows show the flow of data between components.\n"
     ]
    }
   ],
   "source": [
    "# For normal images\n",
    "image_paths = [\n",
    "    r\"C:\\Users\\Anilchoudary R\\Gitrepos\\Multimodal-RAG-Application\\note-books\\extracted_docs\\figure-3-1.jpg\",\n",
    "    r\"C:\\Users\\Anilchoudary R\\Gitrepos\\Multimodal-RAG-Application\\note-books\\extracted_docs\\figure-4-2.jpg\",\n",
    "]\n",
    "\n",
    "image_b64_list, image_summaries = generate_image_based_summaries(image_paths, IMAGE_PROMPT)\n",
    "\n",
    "print(\"Sample image summary:\", image_summaries[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e3d5532a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample table summary: The table presents a comparison of different neural network layer types based on three criteria: Complexity per Layer, Sequential Operations, and Maximum Path Length.\n",
      "\n",
      "### Data Columns:\n",
      "\n",
      "1. **Layer Type**:\n",
      "   - Lists four different types of neural network layers.\n",
      "   - Types: Self-Attention, Recurrent, Convolutional, Self-Attention (restricted).\n",
      "\n",
      "2. **Complexity per Layer**:\n",
      "   - Indicates the computational complexity of each layer type.\n",
      "   - Given as big O notation for each layer:\n",
      "     - Self-Attention: \\(O(n^2 \\cdot d)\\)\n",
      "     - Recurrent: \\(O(n \\cdot d^2)\\)\n",
      "     - Convolutional: \\(O(k \\cdot n \\cdot d^2)\\)\n",
      "     - Self-Attention (restricted): \\(O(r \\cdot n \\cdot d)\\)\n",
      "\n",
      "3. **Sequential Operations**:\n",
      "   - Denotes the number of operations that need to be performed in sequence.\n",
      "   - Values:\n",
      "     - Self-Attention: \\(O(1)\\)\n",
      "     - Recurrent: \\(O(n)\\)\n",
      "     - Convolutional: \\(O(1)\\)\n",
      "     - Self-Attention (restricted): \\(O(1)\\)\n",
      "\n",
      "4. **Maximum Path Length**:\n",
      "   - Refers to the maximum distance in layers for information propagation.\n",
      "   - Values:\n",
      "     - Self-Attention: \\(O(1)\\)\n",
      "     - Recurrent: \\(O(n)\\)\n",
      "     - Convolutional: \\(O(\\log_k(n))\\)\n",
      "     - Self-Attention (restricted): \\(O(n/r)\\)\n",
      "\n",
      "### Key Insights:\n",
      "\n",
      "- **Complexity**: Self-Attention layers have higher complexity than restricted Self-Attention and Convolutional. Recurrent layers have relatively simpler complexity dependent on \\(n\\).\n",
      "- **Sequential Operations**: Recurrent networks require more sequential processing than the other types, which are efficient with \\(O(1)\\).\n",
      "- **Maximum Path Length**: This measure suggests how deep the layers need to be; Self-Attention and Convolutional layers allow shorter path lengths compared to Recurrent layers.\n",
      "\n",
      "This information is useful for comparing the computational efficiency and architectural design implications of different neural network layers.\n"
     ]
    }
   ],
   "source": [
    "# 2️ For table images\n",
    "table_image_paths = [\n",
    "    r\"C:\\Users\\Anilchoudary R\\Gitrepos\\Multimodal-RAG-Application\\note-books\\extracted_docs\\table-6-1.jpg\",\n",
    "    r\"C:\\Users\\Anilchoudary R\\Gitrepos\\Multimodal-RAG-Application\\note-books\\extracted_docs\\table-8-2.jpg\",\n",
    "]\n",
    "\n",
    "table_b64_list, table_summaries = generate_image_based_summaries(table_image_paths, TABLE_PROMPT)\n",
    "\n",
    "print(\"Sample table summary:\", table_summaries[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54e08f4",
   "metadata": {},
   "source": [
    "### Semantic Chunking ( Optional )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a0589c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 47\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "all_texts = NarrativeTexts + Text + ListItem + Title  # + image/table summaries if needed\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(\"\\n\".join(all_texts))\n",
    "print(f\"Total chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7a6bea98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents for embedding: 55\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "text_docs = [Document(page_content=chunk, metadata={\"type\": \"text\"}) for chunk in chunks]\n",
    "table_summary_docs = [Document(page_content=s, metadata={\"type\": \"table_summary\"}) for s in table_summaries]\n",
    "table_docs = [Document(page_content=t, metadata={\"type\": \"table_image\"}) for t in table_b64_list]\n",
    "image_summary_docs = [Document(page_content=s, metadata={\"type\": \"image_summary\"}) for s in image_summaries]\n",
    "image_docs = [Document(page_content=i, metadata={\"type\": \"image_base64\"}) for i in image_b64_list]\n",
    "\n",
    "all_docs = text_docs + table_summary_docs + table_docs + image_summary_docs + image_docs\n",
    "print(f\"Total documents for embedding: {len(all_docs)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0961a93c",
   "metadata": {},
   "source": [
    " ### Setup Qdrant Collections with Different Index Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "50c861b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections created successfully.\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, HnswConfigDiff\n",
    "\n",
    "qdrant_client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "embedding_size = 1536  # OpenAI embedding size\n",
    "\n",
    "# Define collection parameters for flat (default)\n",
    "flat_params = {\n",
    "    \"vectors_config\": VectorParams(size=embedding_size, distance=Distance.COSINE),\n",
    "}\n",
    "\n",
    "# Define collection parameters for HNSW index\n",
    "hnsw_params = {\n",
    "    \"vectors_config\": VectorParams(size=embedding_size, distance=Distance.COSINE),\n",
    "    \"hnsw_config\": HnswConfigDiff(m=16, ef_construct=200),\n",
    "}\n",
    "\n",
    "collections = {\n",
    "    \"mm_flat\": flat_params,\n",
    "    \"mm_hnsw\": hnsw_params,\n",
    "}\n",
    "\n",
    "for col_name, params in collections.items():\n",
    "    try:\n",
    "        qdrant_client.delete_collection(collection_name=col_name)\n",
    "    except Exception:\n",
    "        pass\n",
    "    qdrant_client.create_collection(collection_name=col_name, **params)\n",
    "\n",
    "print(\"Collections created successfully.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "72f213b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading documents to collection mm_flat ...\n",
      "Uploading documents to collection mm_hnsw ...\n",
      "All documents uploaded and embedded.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "\n",
    "embedding_fn = OpenAIEmbeddings()\n",
    "\n",
    "def upload_to_qdrant(collection_name):\n",
    "    vectorstore = Qdrant.from_documents(\n",
    "        documents=all_docs,\n",
    "        embedding=embedding_fn,\n",
    "        url=\"http://localhost:6333\",\n",
    "        collection_name=collection_name,\n",
    "    )\n",
    "    return vectorstore\n",
    "\n",
    "vectorstores = {}\n",
    "for col_name in collections.keys():\n",
    "    print(f\"Uploading documents to collection {col_name} ...\")\n",
    "    vectorstores[col_name] = upload_to_qdrant(col_name)\n",
    "\n",
    "print(\"All documents uploaded and embedded.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ff570",
   "metadata": {},
   "source": [
    "### Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "300ca60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever 'mm_flat' query time: 0.311 seconds\n",
      "Retriever 'mm_hnsw' query time: 0.225 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "retrievers = {}\n",
    "query = \"Explain attention mechanism in the paper.\"\n",
    "\n",
    "for name, vs in vectorstores.items():\n",
    "    # Use the `k` param instead of `search_kwargs`\n",
    "    retriever = vs.as_retriever(k=5)  # ✅ THIS is the proper way\n",
    "    start = time.time()\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    duration = time.time() - start\n",
    "    retrievers[name] = retriever\n",
    "    print(f\"Retriever '{name}' query time: {duration:.3f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae10b5d",
   "metadata": {},
   "source": [
    "### Measure retrieval time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "04adc02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved in 0.289 sec\n",
      "3.2 Attention\n",
      "3.2 Attention\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dep\n",
      "Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dep\n",
      "3.2.3 Applications of Attention in our Model\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "query = \"Explain the attention mechanism\"\n",
    "start = time.time()\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Retrieved in {end - start:.3f} sec\")\n",
    "for d in docs:\n",
    "    print(d.page_content[:150])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f1380c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever accuracy: 0.33\n"
     ]
    }
   ],
   "source": [
    "# Make sure to install sklearn before running this cell: pip install scikit-learn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "queries = [\n",
    "    \"Explain attention\",\n",
    "    \"What is the key result?\",\n",
    "    \"Describe figure 3-1\"\n",
    "]\n",
    "\n",
    "expected_keywords = [\n",
    "    \"attention\",\n",
    "    \"key result\",\n",
    "    \"figure 3\"\n",
    "]\n",
    "\n",
    "correct = []\n",
    "for q, keyword in zip(queries, expected_keywords):\n",
    "    docs = retrievers[\"mm_flat\"].get_relevant_documents(q)  # Use 'mm_flat' or loop all\n",
    "    combined_text = \" \".join([d.page_content.lower() for d in docs])\n",
    "    correct.append(keyword.lower() in combined_text)\n",
    "\n",
    "accuracy = accuracy_score(correct, [True]*len(correct))\n",
    "print(f\"Retriever accuracy: {accuracy:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8e87eeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranked top document preview:\n",
      "Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\n",
      "End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recur\n"
     ]
    }
   ],
   "source": [
    "# Use built-in MMR support in the retriever\n",
    "mmr_retriever = retrievers[\"mm_flat\"].vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 5, \"lambda_mult\": 0.5}\n",
    ")\n",
    "\n",
    "reranked_docs = mmr_retriever.get_relevant_documents(\"Explain attention\")\n",
    "\n",
    "print(f\"Reranked top document preview:\\n{reranked_docs[0].page_content[:500]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fabb522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a multi-modal assistant answering user queries using text and images.\"),\n",
    "    (\"human\", \"\"\"\n",
    "Answer the user's question based on the following retrieved content.\n",
    "\n",
    "## Text:\n",
    "{context}\n",
    "\n",
    "## Question:\n",
    "{input}\n",
    "\"\"\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c58617c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "vision_llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "def separate_text_and_images(docs):\n",
    "    text_chunks = []\n",
    "    image_chunks = []\n",
    "    for doc in docs:\n",
    "        content = doc.page_content\n",
    "        # Basic heuristic: base64 strings likely for images; texts otherwise\n",
    "        if len(content) > 1000 and (content.startswith(\"/9j/\") or content.startswith(\"iVBOR\")):\n",
    "            image_chunks.append(content)\n",
    "        elif content.lower().startswith(\"data:image\") or \"base64\" in content.lower():\n",
    "            image_chunks.append(content)\n",
    "        else:\n",
    "            text_chunks.append(content)\n",
    "    return text_chunks, image_chunks\n",
    "\n",
    "def multi_modal_rag_query(query, retriever):\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    text_chunks, image_chunks = separate_text_and_images(retrieved_docs)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant using text and images.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": rag_prompt.format_messages(context=\"\\n\".join(text_chunks), input=query)[0].content}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for img_b64 in image_chunks:\n",
    "        messages[1][\"content\"].append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_b64}\"}\n",
    "        })\n",
    "\n",
    "    response = vision_llm.invoke(messages)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "cf386157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: These images depict different aspects of the Transformer model, commonly used in natural language processing.\n",
      "\n",
      "1. **Architecture Diagram**: This shows the basic structure of the Transformer model, including the encoder-decoder architecture. Key components include multi-head attention, feed-forward layers, and positional encoding. The encoder and decoder are made up of stacked layers.\n",
      "\n",
      "2. **BLEU Scores and Training Costs**: This table compares the BLEU scores and training costs (measured in FLOPs) of various models on different language translation tasks. The Transformer model is highlighted for its performance and efficiency.\n",
      "\n",
      "3. **Complexity Table**: This table compares different neural network layers in terms of complexity per layer, sequential operations, and maximum path length. It highlights the efficiency of self-attention compared to recurrent and convolutional layers.\n",
      "\n",
      "4. **Scaled Dot-Product Attention**: This diagram explains the process of scaled dot-product attention, a key part of the Transformer's attention mechanism. It involves queries (Q), keys (K), and values (V), and includes steps like matrix multiplication, scaling, masking, and softmax.\n",
      "\n",
      "These elements collectively demonstrate the architecture, efficiency, and attention mechanism of the Transformer model, which revolutionized NLP tasks.\n"
     ]
    }
   ],
   "source": [
    "query = \"What does figure 3-1 show?\"\n",
    "answer = multi_modal_rag_query(query, retrievers[\"mm_flat\"])\n",
    "print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f54d3851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer saved to: generated_answer.docx\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "\n",
    "doc = Document()\n",
    "doc.add_heading(\"LLM Generated Answer\", 0)\n",
    "doc.add_paragraph(answer)\n",
    "\n",
    "output_path = \"generated_answer.docx\"\n",
    "doc.save(output_path)\n",
    "print(f\"\\nAnswer saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d192535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-modal-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
