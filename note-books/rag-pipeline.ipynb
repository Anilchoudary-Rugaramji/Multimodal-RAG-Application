{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a24002d8",
   "metadata": {},
   "source": [
    "### 1. Load Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7781d0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80537a67",
   "metadata": {},
   "source": [
    "### 2. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1f89333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF partitioning\n",
    "from unstructured.partition.pdf import partition_pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5226fc1a",
   "metadata": {},
   "source": [
    "### 3. Partition PDF into Structured Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33524c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<unstructured.documents.elements.Text at 0x27a34edbe70>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edbee0>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edb930>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edb9a0>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edba10>,\n",
       " <unstructured.documents.elements.Header at 0x27a2507c050>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edba80>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edbaf0>,\n",
       " <unstructured.documents.elements.Text at 0x27a3432d470>,\n",
       " <unstructured.documents.elements.Title at 0x27a34edbb60>,\n",
       " <unstructured.documents.elements.Title at 0x27a34edbbd0>,\n",
       " <unstructured.documents.elements.Title at 0x27a34edbc40>,\n",
       " <unstructured.documents.elements.Title at 0x27a345ca190>,\n",
       " <unstructured.documents.elements.Title at 0x27a3432d710>,\n",
       " <unstructured.documents.elements.Title at 0x27a34edbcb0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5b540>,\n",
       " <unstructured.documents.elements.Title at 0x27a2de5b5b0>,\n",
       " <unstructured.documents.elements.Title at 0x27a3432d630>,\n",
       " <unstructured.documents.elements.Title at 0x27a34edbd90>,\n",
       " <unstructured.documents.elements.EmailAddress at 0x27a33fc74d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5b9a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5ba80>,\n",
       " <unstructured.documents.elements.Title at 0x27a34edbd20>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5bd20>,\n",
       " <unstructured.documents.elements.Title at 0x27a34edb620>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4903d0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490210>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490440>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b4904b0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490590>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b490670>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490750>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490f30>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a345ca970>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a34edb690>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4911d0>,\n",
       " <unstructured.documents.elements.Title at 0x27a2de5b3f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5b690>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5b770>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5b930>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5baf0>,\n",
       " <unstructured.documents.elements.Title at 0x27a2de5bcb0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4914e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490050>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490360>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490520>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b4906e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4908a0>,\n",
       " <unstructured.documents.elements.Footer at 0x27a2b490a60>,\n",
       " <unstructured.documents.elements.Image at 0x27a24ea0d10>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x27a25066f90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5b850>,\n",
       " <unstructured.documents.elements.Title at 0x27a2de5bbd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490980>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490280>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b490600>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490b40>,\n",
       " <unstructured.documents.elements.Footer at 0x27a2b490d00>,\n",
       " <unstructured.documents.elements.Title at 0x27a34edb700>,\n",
       " <unstructured.documents.elements.Title at 0x27a345e6040>,\n",
       " <unstructured.documents.elements.Image at 0x27a287f7a50>,\n",
       " <unstructured.documents.elements.Image at 0x27a2f310550>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5ba10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490c20>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b4902f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490de0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490fa0>,\n",
       " <unstructured.documents.elements.Formula at 0x27a24e78ef0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491240>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491400>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b491470>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491550>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4918d0>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edb770>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5b620>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5bd90>,\n",
       " <unstructured.documents.elements.Formula at 0x27a24f85040>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4907c0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491080>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b491320>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4916a0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b491630>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b491b70>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b491c50>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b491d30>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491e10>,\n",
       " <unstructured.documents.elements.Formula at 0x27a24cc2cf0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491da0>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b491e80>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491f60>,\n",
       " <unstructured.documents.elements.Footer at 0x27a2b4923c0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2de5b4d0>,\n",
       " <unstructured.documents.elements.Table at 0x27a24dcbce0>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b4900c0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491160>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4917f0>,\n",
       " <unstructured.documents.elements.Formula at 0x27a2b4fce50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491b00>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491cc0>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b491fd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492430>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492510>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4925f0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4926d0>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edb460>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492740>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491a90>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4915c0>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b491a20>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491ef0>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b4924a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492660>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b492890>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492970>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b492a50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492b30>,\n",
       " <unstructured.documents.elements.Formula at 0x27a2b4ffb50>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492c80>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b492d60>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492e40>,\n",
       " <unstructured.documents.elements.Footer at 0x27a2b492f20>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x27a65b2bf00>,\n",
       " <unstructured.documents.elements.Table at 0x27a292839b0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b490ec0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b491be0>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b492580>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b492900>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492ac0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492c10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492dd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493000>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b4930e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4931c0>,\n",
       " <unstructured.documents.elements.Text at 0x27a345cadd0>,\n",
       " <unstructured.documents.elements.Footer at 0x27a2b493380>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493310>,\n",
       " <unstructured.documents.elements.Table at 0x27a29193bd0>,\n",
       " <unstructured.documents.elements.Title at 0x27a34edb4d0>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edb540>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edb5b0>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edb380>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edb3f0>,\n",
       " <unstructured.documents.elements.Text at 0x27a345e63c0>,\n",
       " <unstructured.documents.elements.Text at 0x27a345e6430>,\n",
       " <unstructured.documents.elements.Text at 0x27a345cb150>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493770>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493850>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493930>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b493a10>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493af0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493bd0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493cb0>,\n",
       " <unstructured.documents.elements.Text at 0x27a34edb7e0>,\n",
       " <unstructured.documents.elements.Text at 0x27a33fcf690>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x27a2b38b9b0>,\n",
       " <unstructured.documents.elements.Table at 0x27a2b38a030>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b492190>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4929e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493070>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b493460>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493620>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4937e0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b4939a0>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493b60>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493e00>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b493ee0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc050>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc130>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc210>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc2f0>,\n",
       " <unstructured.documents.elements.Footer at 0x27a2eebc3d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b493d20>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b4920b0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b493230>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b493700>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b493a80>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b493e70>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc360>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc1a0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc440>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc520>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc600>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc750>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc7c0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc8a0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc980>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebca60>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebcb40>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebcc20>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebcd00>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebcde0>,\n",
       " <unstructured.documents.elements.Footer at 0x27a2eebcec0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b492eb0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b493540>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2b493c40>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebce50>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc280>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc590>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc6e0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebc910>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebcad0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebcc90>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebcf30>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebd010>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebd0f0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebd1d0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebd2b0>,\n",
       " <unstructured.documents.elements.ListItem at 0x27a2eebd390>,\n",
       " <unstructured.documents.elements.Footer at 0x27a2eebd470>,\n",
       " <unstructured.documents.elements.Title at 0x27a2b492cf0>,\n",
       " <unstructured.documents.elements.Image at 0x27a2be4c5f0>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x27a2b4ffe50>,\n",
       " <unstructured.documents.elements.Header at 0x27a24d0cfc0>,\n",
       " <unstructured.documents.elements.Image at 0x27a34008050>,\n",
       " <unstructured.documents.elements.NarrativeText at 0x27a2b493f50>,\n",
       " <unstructured.documents.elements.Text at 0x27a2de5be70>,\n",
       " <unstructured.documents.elements.Image at 0x27a2dea9a90>,\n",
       " <unstructured.documents.elements.Image at 0x27a2dea9630>,\n",
       " <unstructured.documents.elements.FigureCaption at 0x27a2b4fef50>,\n",
       " <unstructured.documents.elements.Header at 0x27a292803b0>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Partition a PDF into elements: headers, paragraphs, images, tables, etc.\n",
    "#  Path to your PDF\n",
    "pdf_path = r\"C:\\Users\\Anilchoudary R\\Gitrepos\\Multimodal-RAG-Application\\data\\attention_paper.pdf\"\n",
    "\n",
    "#  Partition the PDF\n",
    "raw_pdf_elements = partition_pdf(\n",
    "    filename=pdf_path,\n",
    "    strategy=\"hi_res\",\n",
    "    extract_images_in_pdf=True,\n",
    "    extract_image_block_types=[\"Image\", \"Table\"],\n",
    "    extract_image_block_to_payload=False,\n",
    "    extract_image_block_output_dir=\"extracted_docs\"\n",
    ")\n",
    "\n",
    "raw_pdf_elements\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797b20d",
   "metadata": {},
   "source": [
    "### 4. Categorize PDF Elements by Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7860106a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers: 3, Footers: 8, Titles: 40\n",
      "NarrativeTexts: 83, Text: 21, ListItems: 43\n",
      "Tables: 4, Images: 7\n"
     ]
    }
   ],
   "source": [
    "#  Organize extracted PDF elements by type\n",
    "\n",
    "Header, Footer, Title = [], [], []\n",
    "NarrativeTexts, Text, ListItem = [], [], []\n",
    "Images, Tables = [], []\n",
    "\n",
    "for element in raw_pdf_elements:\n",
    "    t = str(type(element))\n",
    "    if \"Header\" in t:\n",
    "        Header.append(str(element))\n",
    "    elif \"Footer\" in t:\n",
    "        Footer.append(str(element))\n",
    "    elif \"Title\" in t:\n",
    "        Title.append(str(element))\n",
    "    elif \"NarrativeText\" in t:\n",
    "        NarrativeTexts.append(str(element))\n",
    "    elif \"Text\" in t:\n",
    "        Text.append(str(element))\n",
    "    elif \"ListItem\" in t:\n",
    "        ListItem.append(str(element))\n",
    "    elif \"Image\" in t:\n",
    "        Images.append(str(element))\n",
    "    elif \"Table\" in t:\n",
    "        Tables.append(str(element))\n",
    "\n",
    "#  Summary of extracted types\n",
    "print(f\"Headers: {len(Header)}, Footers: {len(Footer)}, Titles: {len(Title)}\")\n",
    "print(f\"NarrativeTexts: {len(NarrativeTexts)}, Text: {len(Text)}, ListItems: {len(ListItem)}\")\n",
    "print(f\"Tables: {len(Tables)}, Images: {len(Images)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f08c0",
   "metadata": {},
   "source": [
    "This block loops over all elements and sorts them into lists so you can handle text, tables, and images separately later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d981a6",
   "metadata": {},
   "source": [
    "### 5.  Helper Functions for encoding image binary first into base64 and then into python string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a8d7c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Helper to encode an image to base64\n",
    "import base64\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"\n",
    "    Encode an image as a base64 string.\n",
    "    \"\"\"\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca762d73",
   "metadata": {},
   "source": [
    "### 6. Helper Function for Image Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29acf6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Summarize an image using GPT-4 with vision\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def image_summarize(img_b64, prompt):\n",
    "    \"\"\"\n",
    "    Summarize a base64-encoded image with a text prompt using GPT-4 Vision.\n",
    "    \"\"\"\n",
    "    model = ChatOpenAI(model=\"gpt-4o\", max_tokens=1024)\n",
    "    response = model.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{img_b64}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc359e0",
   "metadata": {},
   "source": [
    "These helper functions encode an image to base64 and then use GPT-4 Vision to summarize it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bed601",
   "metadata": {},
   "source": [
    "### 7. Helper Function for generating Image Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0d0a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_based_summaries(image_paths, prompt):\n",
    "    \"\"\"\n",
    "    Generic helper: encode and summarize a list of image file paths using a given prompt.\n",
    "    Returns: (list of base64 strings, list of summaries)\n",
    "    \"\"\"\n",
    "    b64_list = []\n",
    "    summaries = []\n",
    "\n",
    "    for path in image_paths:\n",
    "        b64 = encode_image(path)\n",
    "        b64_list.append(b64)\n",
    "        summary = image_summarize(b64, prompt)\n",
    "        summaries.append(summary)\n",
    "\n",
    "    return b64_list, summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a892b3",
   "metadata": {},
   "source": [
    "### 8. Prompts for Tables and Images separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e006e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PROMPT = (\n",
    "    \"You are an assistant tasked with summarizing images for retrieval. \"\n",
    "    \"Describe the content of this image in detail.\"\n",
    ")\n",
    "\n",
    "TABLE_PROMPT = (\n",
    "    \"You are an assistant tasked with summarizing tables from images for retrieval. \"\n",
    "    \"Describe the data, key insights, and structure in detail.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb36fb",
   "metadata": {},
   "source": [
    "### 9. Generate Base64 and Summaries for Normal Images and Table Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1785c542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample image summary: The image is a diagram representing the architecture of a Transformer model, commonly used in natural language processing. \n",
      "\n",
      "- **Left Section (Encoder):**\n",
      "  - At the bottom, it starts with \"Input Embedding\" where inputs are embedded.\n",
      "  - Positional encoding is applied before they enter the layers.\n",
      "  - The encoder consists of multiple identical layers (Nx) wrapping two main components: \"Multi-Head Attention\" and \"Feed Forward.\"\n",
      "  - Each component is followed by \"Add & Norm,\" indicating a residual connection and layer normalization.\n",
      "\n",
      "- **Right Section (Decoder):**\n",
      "  - Begins with \"Output Embedding\" and includes shifted outputs. \n",
      "  - Similar to the encoder, positional encoding is added.\n",
      "  - The decoder has more components per layer: \"Masked Multi-Head Attention\", \"Multi-Head Attention\" (taking input from the encoder), and \"Feed Forward,\" each followed by \"Add & Norm.\"\n",
      "  \n",
      "- **Top Section:**\n",
      "  - The final layers include a \"Linear\" transformation followed by a \"Softmax\" to convert model outputs into probabilities.\n",
      "\n",
      "Arrows show the flow of data between components.\n"
     ]
    }
   ],
   "source": [
    "# For normal images\n",
    "image_paths = [\n",
    "    r\"C:\\Users\\Anilchoudary R\\Gitrepos\\Multimodal-RAG-Application\\note-books\\extracted_docs\\figure-3-1.jpg\",\n",
    "    r\"C:\\Users\\Anilchoudary R\\Gitrepos\\Multimodal-RAG-Application\\note-books\\extracted_docs\\figure-4-2.jpg\",\n",
    "]\n",
    "\n",
    "image_b64_list, image_summaries = generate_image_based_summaries(image_paths, IMAGE_PROMPT)\n",
    "\n",
    "print(\"Sample image summary:\", image_summaries[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3d5532a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample table summary: The table compares different types of neural network layers in terms of their computational characteristics. Here are the key components and insights:\n",
      "\n",
      "### Structure:\n",
      "1. **Columns:**\n",
      "   - **Layer Type:** Identifies the type of neural network layer being analyzed.\n",
      "   - **Complexity per Layer:** Describes the computational complexity for each layer type, often influenced by factors such as input size (n), dimension (d), and kernel size (k or r).\n",
      "   - **Sequential Operations:** Determines the number of sequential operations required.\n",
      "   - **Maximum Path Length:** Indicated the maximum number of steps required to combine information from the most distant parts of the layer.\n",
      "\n",
      "2. **Rows:**\n",
      "   - **Self-Attention:** Complexity of \\(O(n^2 \\cdot d)\\), one sequential operation \\(O(1)\\), and maximum path length of \\(O(1)\\). This suggests efficiency in parallelizing operations.\n",
      "   - **Recurrent:** Complexity of \\(O(n \\cdot d^2)\\), more sequential steps \\(O(n)\\), and its maximum path length is \\(O(n)\\). This indicates lower parallelization potential.\n",
      "   - **Convolutional:** Complexity of \\(O(k \\cdot n \\cdot d^2)\\), minimal sequential operations \\(O(1)\\), and maximum path length \\(O(\\log_k(n))\\). This reflects good parallelization with smaller path length growth.\n",
      "   - **Self-Attention (restricted):** Complexity of \\(O(r \\cdot n \\cdot d)\\), it also facilitates parallel processing with \\(O(1)\\) sequential operations and a path length of \\(O(n/r)\\), implying optimized connection handling.\n",
      "\n",
      "### Key Insights:\n",
      "- **Parallelization:** Self-attention and its restricted variant excel in parallel processing due to lower sequential operations.\n",
      "- **Efficiency:** Convolutional layers balance computational demands and efficient path length reduction, useful for scalable applications.\n",
      "- **Sequential Nature:** Recurrent layers necessitate more sequential operations, impacting their speed and scalability relative to the others. \n",
      "\n",
      "Overall, the table provides a clear comparison of how different layer architectures affect computational requirements in terms of complexity, scalability, and efficiency.\n"
     ]
    }
   ],
   "source": [
    "# 2️ For table images\n",
    "table_image_paths = [\n",
    "    r\"C:\\Users\\Anilchoudary R\\Gitrepos\\Multimodal-RAG-Application\\note-books\\extracted_docs\\table-6-1.jpg\",\n",
    "    r\"C:\\Users\\Anilchoudary R\\Gitrepos\\Multimodal-RAG-Application\\note-books\\extracted_docs\\table-8-2.jpg\",\n",
    "]\n",
    "\n",
    "table_b64_list, table_summaries = generate_image_based_summaries(table_image_paths, TABLE_PROMPT)\n",
    "\n",
    "print(\"Sample table summary:\", table_summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7abcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, Collection, utility, FieldSchema, CollectionSchema, DataType\n",
    "\n",
    "# Connect to Milvus\n",
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "# Create a collection function\n",
    "def create_milvus_collection(name, dim, index_type):\n",
    "    if utility.has_collection(name):\n",
    "        Collection(name).drop()\n",
    "    fields = [\n",
    "        FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "        FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=dim)\n",
    "    ]\n",
    "    schema = CollectionSchema(fields)\n",
    "    collection = Collection(name, schema)\n",
    "    collection.create_index(\n",
    "        field_name=\"embedding\",\n",
    "        index_params={\"index_type\": index_type, \"metric_type\": \"COSINE\", \"params\": {\"nlist\": 1024}}\n",
    "    )\n",
    "    collection.load()\n",
    "    return collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88ee13",
   "metadata": {},
   "source": [
    "### 10. Store in Multi-Vector Retriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "1831c523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Store all text/table/image summaries in a Multi-Vector Retriever\n",
    "\n",
    "import uuid\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "#  Create retriever function\n",
    "def create_multi_vector_retriever(vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images):\n",
    "    store = InMemoryByteStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key\n",
    "    )\n",
    "\n",
    "    def add_documents(retriever, summaries, contents):\n",
    "        ids = [str(uuid.uuid4()) for _ in contents]\n",
    "        summary_docs = [Document(page_content=s, metadata={id_key: ids[i]}) for i, s in enumerate(summaries)]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(ids, contents)))\n",
    "\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts)\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables)\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images)\n",
    "\n",
    "    return retriever\n",
    "\n",
    "#  Create vector store\n",
    "vectorstore = Chroma(\n",
    "    collection_name=\"mm_rag\",\n",
    "    embedding_function=OpenAIEmbeddings()\n",
    ")\n",
    "\n",
    "# Create retriever\n",
    "retriever_multi_vector = create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries=None,  # if you have text summaries, use here\n",
    "    texts=Text,\n",
    "    table_summaries=table_summaries,\n",
    "    tables=Tables,\n",
    "    image_summaries=image_summaries,\n",
    "    images=img_b64_list\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39ac16",
   "metadata": {},
   "source": [
    "This cell stores all your summaries + raw content in a retriever — so later you can retrieve relevant content AND the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190850fd",
   "metadata": {},
   "source": [
    "### 9: Helper to Separate Base64 Images & Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "d337fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_text_and_images(docs):\n",
    "    \"\"\"\n",
    "    Separate retrieved docs into plain text and base64 images.\n",
    "    Supports input items that are either Document objects or plain strings.\n",
    "    \"\"\"\n",
    "    text_chunks = []\n",
    "    image_chunks = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Handle both Document objects and plain strings\n",
    "        content = doc.page_content if hasattr(doc, \"page_content\") else str(doc)\n",
    "\n",
    "        # Simple heuristic to detect images as base64 strings\n",
    "        if len(content) > 1000 and content[:5] in [\"/9j/4\", \"iVBOR\"]:\n",
    "            image_chunks.append(content)\n",
    "        elif \"base64\" in content.lower() and len(content) > 500:\n",
    "            image_chunks.append(content)\n",
    "        else:\n",
    "            text_chunks.append(content)\n",
    "\n",
    "    return text_chunks, image_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3794c2d1",
   "metadata": {},
   "source": [
    "### 10.Prompt for Vision Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "695e7b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create the prompt for the GPT-4 Vision model\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a multi-modal assistant answering user queries using both text and images.\"),\n",
    "    (\"human\", \"\"\"\n",
    "Answer the user's question based on the following retrieved content.\n",
    "\n",
    "## Text:\n",
    "{context}\n",
    "\n",
    "## Question:\n",
    "{input}\n",
    "\"\"\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deaf3870",
   "metadata": {},
   "source": [
    "### 11. Define the Rag chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "10d5a3ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: This diagram depicts the architecture of a Transformer model, which is widely used in natural language processing tasks. Here's a brief explanation of its components:\n",
      "\n",
      "1. **Input Embedding and Output Embedding**: \n",
      "   - Inputs and outputs are converted into continuous vector representations.\n",
      "\n",
      "2. **Positional Encoding**: \n",
      "   - Adds information about the position of each word in the sequence since the model doesn't inherently recognize sequence order.\n",
      "\n",
      "3. **Multi-Head Attention**: \n",
      "   - Allows the model to focus on different parts of the input sequence when generating an output, capturing various contextual relationships.\n",
      "\n",
      "4. **Add & Norm**: \n",
      "   - Refers to adding input to the output of the layer and then normalizing it. This helps in stabilizing the learning process.\n",
      "\n",
      "5. **Feed Forward**: \n",
      "   - A fully connected feed-forward network applied independently to each position in the sequence.\n",
      "\n",
      "6. **Masked Multi-Head Attention**: \n",
      "   - Similar to Multi-Head Attention, but prevents positions from attending to subsequent positions. This is used in the decoder to ensure that predictions for the current step do not depend on future outputs.\n",
      "\n",
      "7. **Final Linear and Softmax Layer**: \n",
      "   - Transforms the final output of the decoder stack to produce probabilities over the vocabulary for prediction.\n",
      "\n",
      "8. **Nx Blocks**: \n",
      "   - The encoders and decoders are repeated N times (typically 6 in the original Transformer).\n",
      "\n",
      "The Encoder processes the input sequence, and the Decoder generates the output sequence. This setup is key to tasks like translation or text generation.\n"
     ]
    }
   ],
   "source": [
    "# 📌 Full RAG chain for text + image context\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# LLM with Vision capability\n",
    "vision_llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Text parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# RAG chain: Retrieve → Separate → Format → Invoke Vision LLM\n",
    "def multi_modal_rag_query(query):\n",
    "    # Step 1: Retrieve\n",
    "    retrieved = retriever_multi_vector.invoke(query)\n",
    "    \n",
    "    # Step 2: Separate\n",
    "    text_chunks, image_chunks = separate_text_and_images(retrieved)\n",
    "    \n",
    "    # Step 3: Compose message for LLM\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant using both text and image context.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": rag_prompt.format_messages(context='\\n'.join(text_chunks), input=query)[0].content}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for img_b64 in image_chunks:\n",
    "        messages[1][\"content\"].append({\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{img_b64}\"\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Step 4: Call LLM\n",
    "    response = vision_llm.invoke(messages)\n",
    "    return response.content\n",
    "\n",
    "# ✅ Test it\n",
    "result = multi_modal_rag_query(\"What does figure 3-1 show?\")\n",
    "print(\"Answer:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb522e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-modal-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
